{% extends "base.html" %}

{% block title %}Fine-tuning | Diffusion {% endblock %}

{% block content %}

<div class="py-16 px-4 md:px-8 max-w-6xl mx-auto dark:bg-gray-900 dark:text-gray-100">
  <div>
    <h2 class="text-4xl font-bold mb-4 text-indigo-600 dark:text-indigo-400 text-center">
      Practice Fine-Tuning <br>Diffusion Models
    </h2>

    <p class="mb-4">
      Fine-tuning Diffusion Models is both science and craftsmanship. This guide walks you from the conceptual
      building blocks to hands-on recipes for stabilizing and improving synthesis quality.
      We'll use the <a href="https://github.com/dennys246/coreDiff" class="font-semibold text-indigo-300 hover:text-indigo-600">coreDiff</a>
      and the <a href="/snowpack_dataset" class="font-semibold hover:text-indigo-600">Rocky Mountain snowpack dataset</a> as a concrete example
      by generating snowpack images like this within the dataset.
    </p><br>

  <div class="flex flex-col items-center">
    <img src="{{ url_for('static', filename='img/core_1_split.JPG') }}"
        alt="Synthetic image generated by the coreDiffusor"
        class="mb-4 w-[500px] max-w-full rounded-xl border-4 border-indigo-400 shadow-lg mx-auto block">
    <p class="mt-2 text-sm text-center text-indigo-200 italic">
      Picture of a snowpack core taken apart of the <strong class="text-indigo-300 font-bold">Rocky Mountain Snowpack</strong> dataset.
    </p>
  </div><br><br>
    
    <nav class="mb-8">
      <h2 class="text-xl font-bold">Table of Contents</h2>
      <ul class="list-disc pl-6 space-y-1 text-indigo-400">
        <li><a href="#cli" class="hover:text-indigo-600">Getting Started</a></li>
        <li><a href="#model" class="hover:text-indigo-600">Model Structure</a></li>
        <li><a href="#hyperparameters" class="hover:text-indigo-600">Hyperparameters (with recommended ranges)</a></li>
        <li><a href="#practice" class="hover:text-indigo-600">Hands-On Practice</a></li>
        <li><a href="#finetuning" class="hover:text-indigo-600">Fine-Tuning Strategies</a></li>
        <li><a href="#metrics-monitoring" class="hover:text-indigo-600">Metrics & Monitoring</a></li>
        <li><a href="#troubleshooting" class="hover:text-indigo-600">Troubleshooting</a></li>
        <li><a href="#references" class="hover:text-indigo-600">Further Reading & References</a></li>
      </ul>
    </nav>

    <!-- CLI -->
    <section id="cli" class="mb-8">
      <h1 class="text-indigo-300 text-2xl font-bold text-center">Getting Started</h1>
      <div class="mt-6 space-y-4">
        <p class="font-bold">Start by installing coreDiff, a diffusion backbone ready to be fine-tuned!</p>
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-6 px-6 copy-container relative">
          <button class="copy-btn absolute top-2 right-2 text-xs bg-gray-700 hover:bg-gray-600 px-2 py-1 rounded">Copy</button>
<pre><code>
git clone https://github.com/dennys246/coreDiff.git
cd coreDiff
pip install -e .
</code></pre>
        </div>

        <p class="font-bold">Train from scratch</p>
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-6 px-6 copy-container relative">
          <button class="copy-btn absolute top-2 right-2 text-xs bg-gray-700 hover:bg-gray-600 px-2 py-1 rounded">Copy</button>
<pre><code>
corediff --mode train
</code></pre>
        </div>

        <p class="font-bold">Explore different configuration, e.g. increase U-Net depth and learning rate:</p>
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-6 px-6 copy-container relative">
          <button class="copy-btn absolute top-2 right-2 text-xs bg-gray-700 hover:bg-gray-600 px-2 py-1 rounded">Copy</button>
<pre><code>
corediff --mode train --unet_depth 4 --lr 5e-4
</code></pre>
        </div>

        <p class="font-bold">Generate images with your trained model</p>
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-6 px-6 copy-container relative">
          <button class="copy-btn absolute top-2 right-2 text-xs bg-gray-700 hover:bg-gray-600 px-2 py-1 rounded">Copy</button>
<pre><code>
corediff --mode generate --checkpoint keras/corediff/diffusion.keras --n_samples 64
</code></pre>
        </div>

        <p class="mt-3 text-sm text-gray-600 dark:text-gray-400">
          Run <code>corediff --help</code> to list every tunable hyperparameter, checkpoint flags, and data paths.
        </p>
      </div>
    </section>

    <!-- MODEL -->
    <section id="model" class="mb-8">
      <h1 class="text-indigo-300 text-2xl font-bold text-center">Model Structure</h1>
      <div class="mt-6 space-y-4">
        <h3 class="font-semibold">High-level view</h3>
        <p>
          A diffusion model gradually denoises random noise into data samples using a <strong>U-Net backbone</strong>
          conditioned on timestep embeddings. Fine-tuning adapts the noise schedule, U-Net capacity, and conditioning
          (e.g. class labels, text prompts) to new datasets and tasks.
        </p>

        <h3 class="font-semibold">U-Net (in depth)</h3>
        <ul class="list-disc pl-6">
          <li><strong>Input:</strong> noised image + timestep + optional conditioning.</li>
          <li><strong>Downsampling path:</strong> convolutional blocks with residual connections, GroupNorm, and non-linearities.</li>
          <li><strong>Bottleneck:</strong> multi-head self-attention and residual layers.</li>
          <li><strong>Upsampling path:</strong> mirror of downsampling with skip connections.</li>
          <li><strong>Output:</strong> predicted noise or denoised image depending on parameterization.</li>
        </ul>

        <h3 class="font-semibold">Noise Schedule</h3>
        <ul class="list-disc pl-6">
          <li><strong>Linear / cosine schedules:</strong> control how noise is added during training.</li>
          <li><strong>β range:</strong> defines the variance schedule. Fine-tuning often involves adjusting this for sharper or smoother outputs.</li>
        </ul>
      </div>
    </section>

<!-- HYPERPARAMETERS -->
<section id="hyperparameters" class="mb-8">
  <h1 class="text-indigo-300 text-2xl font-bold text-center">Hyperparameters</h1>
  <div class="mt-6 space-y-4">
    <p class="mb-2">Below are common tunable parameters and guidance for how to change them and why.</p>

    <dl class="divide-y divide-gray-700 bg-gray-800 p-4 rounded">

      <div class="grid grid-cols-4 gap-4 font-semibold text-gray-100 bg-gray-800 p-2 rounded-t">
        <div>Hyperparameter</div>
        <div>CLI call</div>
        <div>Default</div>
        <div>Description</div>
      </div>

      <!-- Model input/output -->
      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Resolution</div>
        <div class="font-mono text-sm text-gray-300">--resolution</div>
        <div class="text-gray-300">[50, 100]</div>
        <div>Downsample images to this resolution (HxW). Smaller = faster, larger = more detail.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Synthetic Samples</div>
        <div class="font-mono text-sm text-gray-300">--n_samples</div>
        <div class="text-gray-300">10</div>
        <div>How many images to generate in a batch of inference.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Batch Size</div>
        <div class="font-mono text-sm text-gray-300">--batch_size</div>
        <div class="text-gray-300">8</div>
        <div>Larger batches stabilize gradients but require more memory. Adjust LR accordingly.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Epochs</div>
        <div class="font-mono text-sm text-gray-300">--epochs</div>
        <div class="text-gray-300">10</div>
        <div>Number of full passes through the dataset during training.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Latent Dim (T)</div>
        <div class="font-mono text-sm text-gray-300">--T</div>
        <div class="text-gray-300">1000</div>
        <div>Dimensionality of the latent space used in diffusion.</div>
      </div>

      <!-- Conv params -->
      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Kernel Size</div>
        <div class="font-mono text-sm text-gray-300">--kernel_size</div>
        <div class="text-gray-300">[4, 4]</div>
        <div>Size of convolutional kernels. Larger kernels capture more context.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Kernel Stride</div>
        <div class="font-mono text-sm text-gray-300">--kernel_stride</div>
        <div class="text-gray-300">[2, 2]</div>
        <div>Stride of convolutional kernels. Larger stride = more downsampling.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Learning Rate</div>
        <div class="font-mono text-sm text-gray-300">--learning_rate</div>
        <div class="text-gray-300">0.001</div>
        <div>Optimizer learning rate. Lower for stability, higher for fast adaptation.</div>
      </div>

      <!-- Diffusion params -->
      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Beta Low</div>
        <div class="font-mono text-sm text-gray-300">--beta_low</div>
        <div class="text-gray-300">1e-5</div>
        <div>Lower bound of diffusion beta schedule. Controls noise scale at start.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Beta High</div>
        <div class="font-mono text-sm text-gray-300">--beta_high</div>
        <div class="text-gray-300">0.02</div>
        <div>Upper bound of diffusion beta schedule. Higher = more aggressive noise at end.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">LeakyReLU Slope</div>
        <div class="font-mono text-sm text-gray-300">--negative_slope</div>
        <div class="text-gray-300">0.25</div>
        <div>Slope for negative inputs in LeakyReLU activations. Controls nonlinearity.</div>
      </div>

      <!-- Architecture params -->
      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Encoder Channels</div>
        <div class="font-mono text-sm text-gray-300">--enc_chs</div>
        <div class="text-gray-300">[64, 128, 256, 512]</div>
        <div>Number of filters per encoder layer (comma-separated). Defines model capacity.</div>
      </div>

      <div class="py-3 grid grid-cols-4 gap-4">
        <div class="font-mono text-sm">Decoder Channels</div>
        <div class="font-mono text-sm text-gray-300">--dec_chs</div>
        <div class="text-gray-300">[512, 256, 128, 64]</div>
        <div>Number of filters per decoder layer (comma-separated). Should mirror encoder.</div>
      </div>

    </dl>

    <p class="mt-3 text-sm text-gray-600 dark:text-gray-400">
      <strong>Rule of thumb:</strong> change one hyperparameter at a time and run for a small number of epochs to see its effect.
      Monitor your resource consumption to check if you need to decrease the size of your Diffusion model and ensure
      you're not overloading you computer.
    </p>
  </div>
</section>

<!-- PRACTICE -->
<section id="practice" class="mb-8">
  <h1 class="text-indigo-300 text-2xl font-bold text-center">Hands-On Practice</h1>
  <div class="mt-6 space-y-4">
    <h3 class="font-semibold">Project: coreDiff — practical recipes</h3>
    <p>Use these stepwise recipes on the Rocky Mountain snowpack dataset.</p>

    <h4 class="font-semibold">Quick sanity run (few epochs)</h4>
    <ol class="list-decimal pl-6">
      <li>Install & clone repository (see Getting Started above).</li>
      <li>Run a 10–20 epoch trial to check pipeline and confirm denoising:
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-4 px-4 copy-container relative my-3">
          <pre><code>corediff --mode train --epochs 20 --batch_size 8 --learning_rate 0.001</code></pre>
        </div>
      </li>
      <li>Inspect generated samples in <code>synthetics/</code> and logs.</li>
    </ol>

    <h4 class="font-semibold">Experimenting with resolution & samples</h4>
    <ol class="list-decimal pl-6">
      <li>Try adjusting the input resolution to balance speed vs. fidelity.</li>
      <li>Generate more synthetic samples per run for evaluation.</li>
      <li>Example:
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-4 px-4 copy-container relative my-3">
          <pre><code>corediff --mode train --resolution '32 64' --n_samples 20</code></pre>
        </div>
      </li>
    </ol>

    <h4 class="font-semibold">Transfer learning & fine-tuning</h4>
    <ol class="list-decimal pl-6">
      <li>Load pretrained checkpoint: <code>--checkpoint keras/corediff/diffusion.keras</code></li>
        <div class="w-full bg-black text-white text-sm font-mono overflow-x-auto py-4 px-4 copy-container relative my-3">
          <pre><code>corediff --mode train --checkpoint keras/corediff/diffusion.keras --learning_rate 5e-4</code></pre>
        </div>
      </li>
      <li>After stable progress, lower LR further or adjust encoder/decoder channels with <code>--enc_chs</code> and <code>--dec_chs</code>.</li>
    </ol>
  </div>
</section>

    <!-- FINETUNING -->
    <section id="finetuning" class="mb-8">
      <h1 class="text-indigo-300 text-2xl font-bold text-center">Fine-Tuning Strategies</h1>
      <div class="mt-6 space-y-4">
        <h3 class="font-semibold">Balance training cost and sample quality</h3>
        <ul class="list-disc pl-6">
          <li><strong>Adjust learning rate:</strong> too high → divergence; too low → slow progress.</li>
          <li><strong>Reduce timesteps:</strong> faster training at some quality tradeoff.</li>
          <li><strong>Use EMA weights:</strong> stabilize inference by maintaining exponential moving averages of parameters.</li>
        </ul>

        <h3 class="font-semibold">Transfer Learning & Warm-Starts</h3>
        <p>
          Instead of training from scratch, initialize from pretrained weights and fine-tune only parts of the U-Net.
        </p>

        <h3 class="font-semibold">Progressive growing & resolution scaling</h3>
        <p>
          Train at lower resolution, then fine-tune at higher resolution. Reduces instability and speeds early learning.
        </p>

        <h3 class="font-semibold">Regularization tricks</h3>
        <ul class="list-disc pl-6">
          <li>Dropout inside U-Net bottleneck.</li>
          <li>Noise augmentation of inputs.</li>
          <li>Smaller guidance scale to avoid overfitting prompts.</li>
        </ul>
      </div>
    </section>

    <!-- REFERENCES -->
    <section id="references" class="mb-12">
      <h1 class="text-indigo-300 text-2xl font-bold text-center">Further Reading & References</h1>
      <div class="mt-6 space-y-3 text-sm">
        <p class="text-gray-700 dark:text-gray-300">
          This guide condenses widely used strategies for diffusion models. For deeper dives:
        </p>
        <ul class="list-disc pl-6">
          <li>Denoising Diffusion Probabilistic Models (Ho et al.)</li>
          <li>Improved Denoising Diffusion (Nichol & Dhariwal)</li>
          <li>Classifier-Free Guidance</li>
          <li>Latent Diffusion Models (Rombach et al.)</li>
          <li>DDIM Sampling</li>
        </ul>
      </div>
    </section>
  </div>
</div>

{% endblock %}